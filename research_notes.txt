# Research Notes: LiteLLM + OpenWebUI + Prompt Caching

## 1. LiteLLM Version
- **Current**: v1.79.3-stable (defined in Dockerfile)
- **Latest Stable**: v1.81.4
- **Action**: Update Dockerfile to use `ghcr.io/berriai/litellm:v1.81.4-stable` (or latest tag).

## 2. OpenWebUI Integration
OpenWebUI connects to LiteLLM as an OpenAI-compatible endpoint.
- **Base URL**: `http://<litellm-ip>:4000/v1` (Note: `/v1` is crucial)
- **API Key**: The value of `LITELLM_MASTER_KEY` (Default: `sk-1234`)
- **Note**: No special config needed on LiteLLM side other than exposing the port, which is already done.

## 3. Anthropic Prompt Caching
LiteLLM supports automatic injection of cache control markers.
- **Param**: `litellm_params.cache_control_injection_points`
- **Configuration**:
  ```yaml
  - model_name: claude-3-5-sonnet
    litellm_params:
      model: anthropic/claude-3-5-sonnet-20240620
      api_key: os.environ/ANTHROPIC_API_KEY
      cache_control_injection_points:
        - system  # Caches the system prompt
        - user    # Can cache user messages if needed, usually system is best for static context
  ```
- **Requirements**:
  - Minimum prompt length (usually 1024 tokens for Anthropic).
  - Cache persists for 5 minutes (default Anthropic behavior).

## 4. Google Gemini Prompt Caching
LiteLLM supports caching for Gemini 1.5 Pro/Flash.
- **Mechanism**: "Context Caching" API.
- **LiteLLM Strategy**: Automatic implementation via simple config or explicit content caching.
- **Configuration**:
  ```yaml
  - model_name: gemini-1.5-pro
    litellm_params:
      model: gemini/gemini-1.5-pro
      api_key: os.environ/GEMINI_API_KEY
      # LiteLLM attempts to auto-cache for Gemini if repeated prompt parts are detected or via explicit params?
      # Research indicates LiteLLM can handle "automatic prompt caching checkpoints".
  ```
- **Note**: The exact config parameter for *automatic* Gemini caching in `config.yaml` might be minimal, often just working out of the box with newer LiteLLM versions, but we should verify if there's a flag like `enable_caching: true` or similar specific to the model.

*Refinement on Gemini*: Search results mentioned "Automatic Prompt Caching" for Gemini in LiteLLM. It seems to analyze the prompt.

## 5. Proposed Config Changes
- Update `config.yaml` to include specific models with caching parameters.
- Define a "standard" set of cached models for the user.

## 6. Verification
- **Build**: `docker-compose build` to pull new image.
- **Run**: `docker-compose up -d`.
- **Test**: Use `curl` to send a request to LiteLLM, checking for headers or response times (second request should be faster).
- **Log check**: LiteLLM logs usually indicate "Cache Hit" or similar.
